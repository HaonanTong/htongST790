---
title: "Homework1"
author: "Haonan Tong"
date: "1/15/2018"

header-includes:
  - \usepackage{bm}
  - \newcommand{\Real}{\mathbb{R}}
  - \newcommand{\dom}{{\bf dom}\,}
  - \newcommand{\tr}{{\bf tr}\,}
  - \newcommand{\Tra}{^{\sf T}} % Transpose
  - \newcommand{\Inv}{^{-1}} % Inverse
  - \def\vec{\mathop{\rm vec}\nolimits}
  - \newcommand{\V}[1]{{\bm{\mathbf{\MakeLowercase{#1}}}}} % vector
  - \newcommand{\VE}[2]{\MakeLowercase{#1}_{#2}} % vector element
  - \newcommand{\Vn}[2]{\V{#1}^{(#2)}} % n-th vector
  - \newcommand{\Vtilde}[1]{{\bm{\tilde \mathbf{\MakeLowercase{#1}}}}} % vector
  - \newcommand{\Vhat}[1]{{\bm{\hat \mathbf{\MakeLowercase{#1}}}}} % vector
  - \newcommand{\VtildeE}[2]{\tilde{\MakeLowercase{#1}}_{#2}} % vector element
  - \newcommand{\M}[1]{{\bm{\mathbf{\MakeUppercase{#1}}}}} % matrix
  - \newcommand{\ME}[2]{\MakeLowercase{#1}_{#2}} % matrix element
  - \newcommand{\Mtilde}[1]{{\bm{\tilde \mathbf{\MakeUppercase{#1}}}}} % matrix
  - \newcommand{\Mbar}[1]{{\bm{\bar \mathbf{\MakeUppercase{#1}}}}} % matrix
  - \newcommand{\Mn}[2]{\M{#1}^{(#2)}} % n-th matrix

output: pdf_document
---
---
title: "Homework 1"
author: "Eric Chi"
date: "Due @ 5pm on January 19, 2018"
header-includes:
  - \usepackage{bm}
  - \newcommand{\Real}{\mathbb{R}}
  - \newcommand{\dom}{{\bf dom}\,}
  - \newcommand{\tr}{{\bf tr}\,}
  - \newcommand{\Tra}{^{\sf T}} % Transpose
  - \newcommand{\Inv}{^{-1}} % Inverse
  - \def\vec{\mathop{\rm vec}\nolimits}
  - \newcommand{\V}[1]{{\bm{\mathbf{\MakeLowercase{#1}}}}} % vector
  - \newcommand{\VE}[2]{\MakeLowercase{#1}_{#2}} % vector element
  - \newcommand{\Vn}[2]{\V{#1}^{(#2)}} % n-th vector
  - \newcommand{\Vtilde}[1]{{\bm{\tilde \mathbf{\MakeLowercase{#1}}}}} % vector
  - \newcommand{\Vhat}[1]{{\bm{\hat \mathbf{\MakeLowercase{#1}}}}} % vector
  - \newcommand{\VtildeE}[2]{\tilde{\MakeLowercase{#1}}_{#2}} % vector element
  - \newcommand{\M}[1]{{\bm{\mathbf{\MakeUppercase{#1}}}}} % matrix
  - \newcommand{\ME}[2]{\MakeLowercase{#1}_{#2}} % matrix element
  - \newcommand{\Mtilde}[1]{{\bm{\tilde \mathbf{\MakeUppercase{#1}}}}} % matrix
  - \newcommand{\Mbar}[1]{{\bm{\bar \mathbf{\MakeUppercase{#1}}}}} % matrix
  - \newcommand{\Mn}[2]{\M{#1}^{(#2)}} % n-th matrix
output: pdf_document
---

**Part 1.**

1. Let $\V{v} \in \Real^{n}$. What is the computational complexity of solving the linear system $\M{A}\V{x} = \V{b}$ where $\M{A} = \M{I} + \V{v}\V{v}\Tra$ if one were to form the matrix $\M{A}$ and then use the $QR$ decomposition as an intermediate step? Include the cost of constructing $\M{A}$ explicitly.

**Answer:**

2. Use the Sherman-Morrison-Woodbury formula (sometimes called the Matrix Inversion Lemma) to solve the linear system in question 1 more efficiently.

**Answer:**

\newpage
3. Let $f : \Real^n \mapsto \Real$ that is differentiable and convex. Let $X$ be an $n$-dimensional random vector whose first moment exists. Prove that

$$
f(\mathbb{E}[X]) \leq \mathbb{E}[f(X)].
$$

Note that it is not necessary to assume that $f$ is differentiable but we'll assume it here to keep things terra cognita. **Hint:** use the fact that the best first order approximation to a convex function is a global underestimator of it.

**Answer:**

This is a proof for Jensen's Inequality. Define $\mathbb{E}[X]=\sum_{i=1}^nt_ix_i$ where $t_i$ are nonnegative real numbers such that $\sum_{i=1}^nt_i=1$.

Proof by induction: The case for $X \in \Real^2$ is true by the definition of convex function $f$.
Assume the relation holds for $X \in \Real^n$ then for the case $X \in \Real^{n+1}$ we have:
$$
\begin{aligned}
f(\mathbb{E}[X]) &= f\left(\sum_{i=1}^{n+1}t_ix_i\right)\\
 &= f\left(\sum_{i=1}^{n}t_ix_i+t_{n+1}x_{n+1}\right)\\
 &= f\left((1-t_{n+1})\frac{1}{1-t_{n+1}} \sum_{i=1}^{n}t_ix_i+t_{n+1}x_{n+1}\right) \\
 &\leq (1-t_{n+1})f\left(\frac{1}{1-t_{n+1}}\sum_{i=1}^{n}t_ix_i\right) + t_{n+1}f(x_{n+1})\\
 &= (1-t_{n+1})f\left(\sum_{i=1}^{n}\frac{t_i}{1-t_{n+1}}x_i\right) + t_{n+1}f(x_{n+1})\\
 &\leq (1-t_{n+1})\sum_{i=1}^{n}\frac{t_i}{1-t_{n+1}}f(x_i) + t_{n+1}f(x_{n+1})\\
 &=\sum_{i=1}^{n}t_if(x_i) + t_{n+1}f(x_{n+1})\\
 &=\sum_{i=1}^{n+1}t_if(x_i)\\
 &=\mathbb{E}[f(X)]\\
\end{aligned}
$$
\newpage
4. The Earth Mover's distance or Wasserstein metric is a distance or metric used to compare two probability distributions. See Levina and Bickel 2001 and references therein. Suppose we have two discrete probability distributions $\V{p}$ and $\V{q}$ on the integers $\{1, 2, \ldots, n\}$. So, $\VE{p}{i}$ and $\VE{q}{i}$ are both nonnegative for all $i \in \{1, \ldots, n\}$ and $\V{p}\Tra\V{1} = \V{q}\Tra\V{1} = 1$, where $\V{1}$ is a vector all ones of length $n$. Then we can quantify the distance between $\V{p}$ and $\V{q}$ by the least amount of work it takes to reshape the distribution $\V{p}$ into the distribution $\V{q}$. Let $d(\V{p},\V{q})$ denote the Earth Mover's distance between $\V{p}$ and $\V{q}$.

$$
d(\V{p}, \V{q}) = \min_{f_{ij}}\; \sum_{i=1}^n\sum_{j=1}^n f_{ij}d_{ij},
$$
subject to
$$
\begin{aligned}
\VE{f}{ij} & \geq 0, \quad \forall i, j \\
\sum_{j=1}^n \VE{f}{ij} & \leq \VE{p}{i}, \quad \forall i \\
\sum_{i=1}^n \VE{f}{ij} & \leq \VE{q}{j}, \quad \forall j \\
\sum_{i=1}^n\sum_{j=1}^n \VE{f}{ij} & = 1.
\end{aligned}
$$

The $d_{ij}$ are given. These are non-negative distances between $i$ and $j$. The $\VE{f}{ij}$ quantify how much probability we are moving from $\VE{p}{i}$ into $\VE{q}{j}$. Thus, the product $\VE{f}{ij}\VE{d}{ij}$ is the amount of work it takes to move probability mass from $\VE{p}{i}$ into $\VE{q}{j}$. Show that the Earth Mover's distance $d(\V{p},\V{q})$ is convex in $(\V{p},\V{q}) \in [0,1]^{2n}$.

**Answer:**

\newpage

**Part 2.** Check your gradient formula

You'll very likely need to derive analytical expressions for gradients when writing iterative optimization algorithms. It's very handy to do a numerical check to have some confidence that you derived things carefully. You will add an implementation of a numerical check of a gradient calculation to your R package.

Let $f(\V{x})$ be a multivariate function that takes in a $p$-dimensional vector $\V{x}$ and maps it to a real number. A more compact way to say this is
$f : \Real^p \mapsto \Real$. We want to check whether we have derived the right formula for the gradient $\nabla f$ of $f$. We do this in the following steps.

1. For fixed vectors $\V{a}, \V{b} \in \Real^p$, define a univariate function $g(t) = f(\V{a} + \V{b}t)$. By the chain rule
$$
g'(t) = \langle \nabla f(\V{a} + \V{b}t), \V{b} \rangle,
$$
and therefore the directional derivative of $f$ at $\V{a}$ in the direction of $\V{b}$ is given by $g'(0)$, namely
$$
df_{\V{b}}(\V{a}) = \left \langle \nabla f(\V{a}), \V{b} \right \rangle.
$$

2. We compare our formula for $g'(t)$ above with the approximation to its derivative
$$
\widehat{df}_{\V{b}}(\V{a},h) = \frac{g(h) - g(0)}{h} = \frac{f(\V{a} + h\V{b}) - f(\V{a})}{h}.
$$


Please complete the following steps.

**Step 0:** Make an R package entitled "unityidST790".
Install packages devtools and roxygen2 that will be needed to create a package.

```{r eval=FALSE}
install.packages("devtools")
install.packages("roxygen2")
library("devtools")
library(roxygen2)
setwd("Google Drive/NOTES/ST790/htongST790")
```


**Step 1:** Write a function "dd_exact."

```{r, echo=TRUE}
#' Directional Derivative Exact
#'
#' \code{dd_exact} computes the exact directional derivative of the multivariate
#' function f, at the point a, in the direction b.
#'
#' @param gradf handle to function that returns the gradient of the function f
#' @param a point at which the directional derivative is evaluated
#' @param b the direction vector
#' @export
dd_exact <- function(gradf, a, b) {

}
```
Your function should return $df_{\V{b}}(\V{a})$.

**Step 2:** Write a function "dd_approx."

```{r, echo=TRUE}
#' Directional Derivative Approximate
#'
#' \code{dd_exact} computes an approximate directional derivative of the multivariate
#' function f, at the point a, in the direction b.
#'
#' @param f handle to function that returns the function f
#' @param a point at which the directional derivative is evaluated
#' @param b the direction vector
#' @param h small displacement
#' @export
dg_approx <- function(f, a, b, h = 1e-13) {
  
}
```
Your function should return $\widehat{df}_{\V{b}}(\V{a},h)$.

**Step 3:** Let $\M{X} \in \Real^{n \times n}$ be a positive definite matrix and let $f(\M{X}) = \log \det (\M{X})$ be a multivariate function defined over the space of positive definite matrices. Someone told you 
that the gradient of $f$ is given by

$$
\nabla f(\M{X}) = -\M{X}\Inv.
$$

Let's check this formula using the two functions you just wrote. Generate 100 pairs of random positive definite matrices $\M{a}$ and random direction matrices $\M{B}$ that are also positive definite. Compute the exact directional derivatives and approximate directional derivative and create a scatter plot of these pairs of 100 points (exact calculatons versus approximate calculations). Does the formula look correct? If so, why? If not, why and how should you correct the formula?

Comments:

- Take the inner product between two $n$-by-$n$ matrices $\M{U}$ and $\M{V}$ as $\langle \M{U}, \M{V} \rangle = \tr(\M{U}\Tra\M{V})$.
- The first thing you should do is figure out how you're going to generate a random positive definite matrix.

```{r scatter plot of exact versus approximate directional derivatives, echo=FALSE, fig.width=6, fig.height=3}
set.seed(12345)
# Put your code here for generating random positive definite matrices
```

